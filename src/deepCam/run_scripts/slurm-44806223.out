:::MLLOG {"namespace": "", "time_ms": 1627361068168, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "deepcam", "metadata": {"file": "/global/u2/k/kwf5687/deepcam/mlperf-deepcam/src/deepCam/utils/mlperf_log_utils.py", "lineno": 55}}
:::MLLOG {"namespace": "", "time_ms": 1627361070483, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Umbrella Corp.", "metadata": {"file": "/global/u2/k/kwf5687/deepcam/mlperf-deepcam/src/deepCam/utils/mlperf_log_utils.py", "lineno": 58}}
:::MLLOG {"namespace": "", "time_ms": 1627361070486, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/global/u2/k/kwf5687/deepcam/mlperf-deepcam/src/deepCam/utils/mlperf_log_utils.py", "lineno": 61}}
:::MLLOG {"namespace": "", "time_ms": 1627361070488, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/global/u2/k/kwf5687/deepcam/mlperf-deepcam/src/deepCam/utils/mlperf_log_utils.py", "lineno": 64}}
:::MLLOG {"namespace": "", "time_ms": 1627361070491, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "32xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/global/u2/k/kwf5687/deepcam/mlperf-deepcam/src/deepCam/utils/mlperf_log_utils.py", "lineno": 67}}
:::MLLOG {"namespace": "", "time_ms": 1627361070493, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 109}}
:::MLLOG {"namespace": "", "time_ms": 1627361070494, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": null, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 110}}
:::MLLOG {"namespace": "", "time_ms": 1627361070496, "event_type": "POINT_IN_TIME", "key": "seed", "value": 333, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 114}}
:::MLLOG {"namespace": "", "time_ms": 1627361070504, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 64, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 187}}
:::MLLOG {"namespace": "", "time_ms": 1627361070506, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "AdamW", "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 188}}
:::MLLOG {"namespace": "", "time_ms": 1627361070508, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.004, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 189}}
:::MLLOG {"namespace": "", "time_ms": 1627361070510, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 190}}
:::MLLOG {"namespace": "", "time_ms": 1627361070511, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 4.0, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 191}}
:::MLLOG {"namespace": "", "time_ms": 1627361070513, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-08, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 192}}
Constructing DeepLabv3+ model...
Number of output channels: 3
Output stride: 16
Number of Input Channels: 16
Initialized dataset with  384  samples.
Initialized dataset with  51  samples.
:::MLLOG {"namespace": "", "time_ms": 1627361075929, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 384, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 309}}
:::MLLOG {"namespace": "", "time_ms": 1627361075934, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 51, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 314}}
:::MLLOG {"namespace": "", "time_ms": 1627361075942, "event_type": "POINT_IN_TIME", "key": "invalid_submission", "value": null, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 318}}
:::MLLOG {"namespace": "", "time_ms": 1627361076294, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 335}}
:::MLLOG {"namespace": "", "time_ms": 1627361076296, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 336}}
:::MLLOG {"namespace": "", "time_ms": 1627361076298, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "../train_hdf5_ddp.py", "lineno": 342, "epoch_num": 1, "step_num": 0}}
../train_hdf5_ddp.py:374: RuntimeWarning: divide by zero encountered in long_scalars
  if (step % pargs.training_visualization_frequency == 0) and (comm_rank == 0):
Traceback (most recent call last):
  File "../train_hdf5_ddp.py", line 581, in <module>
    main(pargs)
  File "../train_hdf5_ddp.py", line 389, in main
    viz.plot(filename[sample_idx], outputfile, plot_input, plot_prediction, plot_label)
UnboundLocalError: local variable 'viz' referenced before assignment
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 44806223.0 ON nid03166 CANCELLED AT 2021-07-26T21:49:08 DUE TO TIME LIMIT ***
srun: got SIGCONT
